atari:
  env_wrapper:
    - stable_baselines3.common.atari_wrappers.AtariWrapper
  frame_stack: 4
  policy: 'CnnPolicy'
  n_envs: 16
  n_timesteps: !!float 1e7
  ent_coef: 0.01
  vf_coef: 0.25


LunarLander-v2:
  n_envs: 8
  n_timesteps: !!float 2e5
  policy: 'MlpPolicy'
  gamma: 0.995
  n_steps: 5
  learning_rate: lin_0.00083
  ent_coef: 0.00001

MountainCar-v0:
  normalize: true
  n_envs: 16
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  ent_coef: .0

Acrobot-v1:
  normalize: true
  n_envs: 16
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  ent_coef: .0

# Almost tuned
Pendulum-v0:
  normalize: True
  n_envs: 8
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_7e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
LunarLanderContinuous-v2:
  normalize: true
  n_envs: 4
  n_timesteps: !!float 5e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_7e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
MountainCarContinuous-v0:
  # env_wrapper: utils.wrappers.PlotActionWrapper
  normalize: true
  n_envs: 4
  n_steps: 100
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  ent_coef: 0.0
  use_sde: True
  sde_sample_freq: 16
  policy_kwargs: "dict(log_std_init=0.0, ortho_init=False)"

CartPole-v1:
  n_envs: 1 # 8
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  #ent_coef: 0.0
  n_steps: 500

# Not Tuned
# --------------------------
ransim-v1:
#  env_wrapper: utils.wrappers.PlotActionWrapper
  normalize: false
  n_envs: 1
  n_steps: 100
  n_timesteps: 1000
  #!!float 1e5
  policy: 'MlpPolicy'
  ent_coef: 2.7067466603177066e-05
#  use_sde: True
#  sde_sample_freq: 16
  learning_rate: 0.007 # new by msa
#  env_kwargs: "dict(t_final=200)"
# --------------------------
ransim-v0:
#  env_wrapper: utils.wrappers.PlotActionWrapper
  normalize: false
  n_envs: 1
  n_steps: 500 #100
  n_timesteps: 30000 #!!float 1e5
  policy: 'MlpPolicy'
  ent_coef: 2.7067466603177066e-05
#  use_sde: True
#  sde_sample_freq: 16
  learning_rate: 0.007 # new by msa
#  env_kwargs: "dict(t_final=200)"
# --------------------------
#ransim-v0:
#  n_envs: 1
#  policy: 'MlpPolicy'
#  n_timesteps: 10000
#  gamma: 0.999
#  normalize_advantage: False #True
#  max_grad_norm: 5
#  use_rms_prop: True
#  gae_lambda: 0.99
#  n_steps: 10
#  #lr_schedule: constant
#  learning_rate: 0.15003251015359345
#  ent_coef: 0.1  #2.7067466603177066e-05
#  vf_coef: 0.20021150651496789
#  #log_std_init: 0.20964555936750617
#  #ortho_init: False
#  #net_arch: small
#  #sde_net_arch: None
#  #full_std: False
#  #activation_fn: relu
# --------------------------
#ransim-v0:
#  n_envs: 1
#  n_steps: 100
#  policy: 'MlpPolicy'
#  n_timesteps: 20000
##Number of finished trials:  30
##Best trial:
##Value:  55.79999923706055
##Params:
#  gamma: 0.99
#  normalize_advantage: True
#  max_grad_norm: 1
#  use_rms_prop: True
#  gae_lambda: 0.95
##  lr_schedule: constant
#  learning_rate: 0.4077422930722311
#  ent_coef: 1.4107772864859497e-05
#  vf_coef: 0.9014667522976627
##Writing report to logs/a2c/report_ransim-v0_30-trials-20000-tpe-median_1593587830.csv
# --------------------------

# Tuned
BipedalWalker-v3:
  normalize: true
  n_envs: 16
  n_timesteps: !!float 5e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
BipedalWalkerHardcore-v3:
  normalize: true
  n_envs: 32
  n_timesteps: !!float 20e7
  policy: 'MlpPolicy'
  ent_coef: 0.001
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.0008
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
HalfCheetahBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  # Both works
  learning_rate: lin_0.00096
  # learning_rate: !!float 3e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False, full_std=True)"

Walker2DBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# normalize: true
# n_envs: 4
# n_timesteps: !!float 2e6
# policy: 'MlpPolicy'
# ent_coef: 0.0
# max_grad_norm: 0.5
# n_steps: 32
# gae_lambda: 0.9
# vf_coef: 0.4
# gamma: 0.99
# use_rms_prop: True
# normalize_advantage: False
# learning_rate: 0.0002
# use_sde: True
# policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
AntBulletEnv-v0:
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
HopperBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned but unstable
# Not working without SDE?
ReacherBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.0008
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"
